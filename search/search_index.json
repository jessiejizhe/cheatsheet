{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Techie A collection of Techie concepts GitHub Page Whenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files. Theme used: MKDocs python -m pip install --upgrade pip pip install mkdocs Make changes and test in browser mkdocs serve http://127.0.0.1:8000/ Generate a /site directory and publish to github pages mkdocs build mkdocs gh-deploy Markdown Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for Syntax highlighted code block # Header 1 ## Header 2 ### Header 3 - Bulleted - List 1. Numbered 2. List **Bold** and _Italic_ and `Code` text [Link](url) and ![Image](src) For more details see GitHub Flavored Markdown .","title":"Home"},{"location":"#techie","text":"A collection of Techie concepts","title":"Techie"},{"location":"#github-page","text":"Whenever you commit to this repository, GitHub Pages will run Jekyll to rebuild the pages in your site, from the content in your Markdown files. Theme used: MKDocs python -m pip install --upgrade pip pip install mkdocs Make changes and test in browser mkdocs serve http://127.0.0.1:8000/ Generate a /site directory and publish to github pages mkdocs build mkdocs gh-deploy","title":"GitHub Page"},{"location":"#markdown","text":"Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for Syntax highlighted code block # Header 1 ## Header 2 ### Header 3 - Bulleted - List 1. Numbered 2. List **Bold** and _Italic_ and `Code` text [Link](url) and ![Image](src) For more details see GitHub Flavored Markdown .","title":"Markdown"},{"location":"checklist/","text":"Techie [ ] DevOps [ ] operating system [ ] file system [ ] developer environment [ ] git [ ] ssh [ ] vim [ ] Regex [ ] CI/CD [ ] Docker [ ] Data Formats [ ] avro [ ] csv [ ] parquet [ ] xml [ ] json [ ] lz4 [ ] orc [ ] data/dat [ ] Database [ ] Druid [ ] Redis [ ] Database Engine [ ] MySQL [ ] NoSQL [ ] PostgreSQL [ ] MongoDB [ ] SQLlite [ ] Oracle SQL [ ] Redshift [ ] Presto [ ] Hive [ ] Langugaes [ ] Bash [ ] Python [ ] Java [ ] JavaScript [ ] HTML [ ] CSS [ ] Apache [ ] Hadoop [ ] YARN [ ] Airflow [ ] DataFu [ ] Druid [ ] Flume [ ] Hive [ ] Kafka [ ] Maven [ ] Oozie [ ] Pig [ ] Phoenix? [ ] Spark [ ] Storm [ ] Stream [ ] Superset [ ] Traffic System (ATS) [ ] Zeppelin [ ] Zookeeper [ ] AWS [ ] Redshift [ ] Statistics [ ] Hypothesis Test [ ] A/B Testing [ ] Experiment Design [ ] Machine Learning [ ] Linear Regression [ ] Logistic Regression [ ] Random Forest [ ] Data Structure [ ] Array [ ] Linked List [ ] Tuple / Struct [ ] Hash Table / Dictionary [ ] Stack [ ] Queue [ ] Heap [ ] Tree [ ] Graph [ ] Matrix [ ] Algorithms [ ] divide and conquer [ ] string manipulation [ ] two patterns [ ] searching (binary, BFS, DFS) [ ] sorting (quick, merge, bubble, heap, bucket, counting, selection, insertion) [ ] bit manipulations [ ] dynamic programming [ ] greedy [ ] hashing [ ] backtracking","title":"Checklist"},{"location":"checklist/#techie","text":"[ ] DevOps [ ] operating system [ ] file system [ ] developer environment [ ] git [ ] ssh [ ] vim [ ] Regex [ ] CI/CD [ ] Docker [ ] Data Formats [ ] avro [ ] csv [ ] parquet [ ] xml [ ] json [ ] lz4 [ ] orc [ ] data/dat [ ] Database [ ] Druid [ ] Redis [ ] Database Engine [ ] MySQL [ ] NoSQL [ ] PostgreSQL [ ] MongoDB [ ] SQLlite [ ] Oracle SQL [ ] Redshift [ ] Presto [ ] Hive [ ] Langugaes [ ] Bash [ ] Python [ ] Java [ ] JavaScript [ ] HTML [ ] CSS [ ] Apache [ ] Hadoop [ ] YARN [ ] Airflow [ ] DataFu [ ] Druid [ ] Flume [ ] Hive [ ] Kafka [ ] Maven [ ] Oozie [ ] Pig [ ] Phoenix? [ ] Spark [ ] Storm [ ] Stream [ ] Superset [ ] Traffic System (ATS) [ ] Zeppelin [ ] Zookeeper [ ] AWS [ ] Redshift [ ] Statistics [ ] Hypothesis Test [ ] A/B Testing [ ] Experiment Design [ ] Machine Learning [ ] Linear Regression [ ] Logistic Regression [ ] Random Forest [ ] Data Structure [ ] Array [ ] Linked List [ ] Tuple / Struct [ ] Hash Table / Dictionary [ ] Stack [ ] Queue [ ] Heap [ ] Tree [ ] Graph [ ] Matrix [ ] Algorithms [ ] divide and conquer [ ] string manipulation [ ] two patterns [ ] searching (binary, BFS, DFS) [ ] sorting (quick, merge, bubble, heap, bucket, counting, selection, insertion) [ ] bit manipulations [ ] dynamic programming [ ] greedy [ ] hashing [ ] backtracking","title":"Techie"},{"location":"data/","text":"Data Serialization Data Formats avro Apache Avro\u2122 is a data serialization system. Avro stores the data definition in JSON format. Avro relies on schema, to have smaller serialization size, and to handle schema changes like missing fields, added fields and changed fields; as a result, old programs can read new data and new programs can read old data. csv A comma-separated values ( CSV ) file is a delimited text file that uses a comma) to separate values. A CSV file typically stores tabular data (numbers and text) in plain text, in which case each line will have the same number of fields. data / dat .data or .dat is a file containing pertinent information about a program or another file. json lz4 orc parquet xml Databases Druid Redis Database Engines Hive MySQL MongoDB NoSQL Oracle PostgreSQL Redshift SQLlite","title":"Data Serialization"},{"location":"data/#data-serialization","text":"","title":"Data Serialization"},{"location":"data/#data-formats","text":"","title":"Data Formats"},{"location":"data/#avro","text":"Apache Avro\u2122 is a data serialization system. Avro stores the data definition in JSON format. Avro relies on schema, to have smaller serialization size, and to handle schema changes like missing fields, added fields and changed fields; as a result, old programs can read new data and new programs can read old data.","title":"avro"},{"location":"data/#csv","text":"A comma-separated values ( CSV ) file is a delimited text file that uses a comma) to separate values. A CSV file typically stores tabular data (numbers and text) in plain text, in which case each line will have the same number of fields.","title":"csv"},{"location":"data/#data-dat","text":".data or .dat is a file containing pertinent information about a program or another file.","title":"data / dat"},{"location":"data/#json","text":"","title":"json"},{"location":"data/#lz4","text":"","title":"lz4"},{"location":"data/#orc","text":"","title":"orc"},{"location":"data/#parquet","text":"","title":"parquet"},{"location":"data/#xml","text":"","title":"xml"},{"location":"data/#databases","text":"","title":"Databases"},{"location":"data/#druid","text":"","title":"Druid"},{"location":"data/#redis","text":"","title":"Redis"},{"location":"data/#database-engines","text":"","title":"Database Engines"},{"location":"data/#hive","text":"","title":"Hive"},{"location":"data/#mysql","text":"","title":"MySQL"},{"location":"data/#mongodb","text":"","title":"MongoDB"},{"location":"data/#nosql","text":"","title":"NoSQL"},{"location":"data/#oracle","text":"","title":"Oracle"},{"location":"data/#postgresql","text":"","title":"PostgreSQL"},{"location":"data/#redshift","text":"","title":"Redshift"},{"location":"data/#sqllite","text":"","title":"SQLlite"},{"location":"Apache/HDFS/","text":"HDFS Overview Design Concept Scalable distributed filesystem Distribute data on local disks on several nodes Low cost commodity hardware Design Factors Hundreds/Thousands of nodes Portability across heterogeneous hardware/software Handle large data sets High throughput Approach Simplified coherency model \u2013 write once read many Data Replication \u2013 helps handle hardware failures Move computation close to data Relax POSIX requirements \u2013 increase throughput POSIX - Portable Operating System Interface, an IEEE standard designed to facilitate application portability Summary Single NameNode a master server that manages the file system namespace and regulates access to files by clients Multiple DataNotes typically one per node in the cluster Functions Manage storage Serving read/write requests from clients Block creation, deletion, replication based on instructions from NameNode HDFS -> HDFS2 HDFS has single NameNode and multiple DataNodes. HDFS2 improvements: HDFS Federation benefits increased namespace scalability performance isolation Multiple Namenode servers Multiple namespaces High Availability (redundant NameNodes) Heterogeneous Storage and Archival Storage (ARCHIVE, DISK, SSD, RAM_DISK) Write Process write process is initiated by a client Data gets cached on the client NameNode contacted once a block of data is accumulated NameNode responds with list of DataNodes NameNode is Rack aware Rack awareness is the knowledge of network structure(topology). i.e. location of different data node across the Hadoop cluster. 1stDataNode receives data, writes to local and forwards to 2nd DataNode ... NameNode commits file creation into persistent store. NameNode receives heartbeat and block reports from all DataNodes. Read Process Client gets DataNode list from NameNode Read from replica closest to reader Performance Envelope HDFS block size default block size is 64MB good for large files Importance of #blocks in a file NameNode memory usage Every block represented as object (default replication this will be further increased 3X) Number of map tasks data typically processed block at a time A lot of #small files Impact on NameNode Memory usage ~150 bytes per object 1 billion objects => 300GB memory! Network load Number of checks with DataNodes proportional to number of blocks Performance Impact Map tasks depends on #blocks 10GB of data, 32k file size => 327680 map tasks \u21d2lots of queued tasks \u21d2large overhead of spin up/tear down for each task (latency) \u21d2Inefficient disk I/O with small sizes Large files lots of small files is bad! Solutions Merge/Concatenate files Sequence files HBase, HIVE configuration CombineFileInputFormat optimizes maps Tuning Parameters Parameters in hdfs-site.xml Block Size default 64MB typically bumped up to 128MB parameter: dfs.blocksize, dfs.block.size Replication default is 3 parameter: dfs.replication tradeoffs Lower it to reduce replication cost, less robust Higher replication can make data local to more workers node count map task Full list dfs.datanode.handler.count (10): Sets the number of server threads on each datanode. dfs.namenode.fs-limits.max-blocks-per-file: Maximum number of blocks per file. Robustness NameNode receives heartbeat and block reports from DataNodes Replication trade off w.r.t robustness Might lose a node or local disk during the run \u2013 cannot recover if there is no replication. If there is data corruption of a block from one of the DataNodes \u2013 again cannot recover without replication. Common Failures & Mitigations Common Failures DataNode Failures: Server can fail, disk can crash, data corruption. Network Failures NameNode Failures: Disk failure, node failure Mitigation of Common Failures Periodic heartbeat: from DataNode to NameNode. For DataNodes without recent heartbeat Marked dead, no new IO sent Blocks below replication factor re-replicated on other nodes. Data Corruption Checksum computed on file creation Checksums stored in HDFS namespace. Used to check retrieved data, re-read from alternate replica if need. Multiple copies of central meta data structures. Failover to standby NameNode \u2013 manual by default. HDFS Access HDFS command invoked via bin/hdfs script user commands - filesystem shell commands administrator commands debug commands HDFS NFS Gateway Mount HDFS as a filesystem on the client Browse files using regular filesystem commands Upload/download files from HDFS Stream data to HDFS Apache Flume collecting, aggregating streaming data and moving into HDFS Apache Sqoop Bulk transfers between Hadoop and datastores. HDFS APIs Native Java API for HDFS** Base class : org.apache.hadoop.fs.FileSystem Important classes FSDataInputStream read : read bytes readFully : read from stream to buffer seek: seek to given offset getPos: get current position in stream FSDataOutputStream getPos: get current position in stream hflush: flush out the data in client's user buffer close: close the underlying output stream Methods : get, open, create Reading from HDFS using API get an instance of FileSystem FileSystem fs = FileSystem.get(URI.create(uri),conf); Open an input stream in = fs.open(new Path(uri)); Use IO utilities to copy from input stream IOUtils.copyBytes(in, System.out,4096,false); Close the stream IOUtils.closeStream(in) Writing to HDFS using API get an instance of FileSystem FileSystem fs = FileSystem.get(URI.create(outuri),conf); Create a file out = fs.create(new Path(outuri)); Write to output stream out.write(buffer, 0, nbytes); Close the file out.close(); C API for HDFS libhdfs, header file (hdfs.h) WebHDFS REST API Enabling WebHDFS in hdfs-site.xml dfs.webhdfs.enabled dfs.web.authentication.kerberos.principal dfs.web.authentication.kerberos.keytab HTTP Operations HTTP GET: file status, checksums, attributes HTTP PUT: create, change ownership, rename, permissions,snapshot HTTP POST: append, concat HTTP DELETE: Delete files, snapshot","title":"HDFS"},{"location":"Apache/HDFS/#hdfs","text":"","title":"HDFS"},{"location":"Apache/HDFS/#overview","text":"Design Concept Scalable distributed filesystem Distribute data on local disks on several nodes Low cost commodity hardware Design Factors Hundreds/Thousands of nodes Portability across heterogeneous hardware/software Handle large data sets High throughput Approach Simplified coherency model \u2013 write once read many Data Replication \u2013 helps handle hardware failures Move computation close to data Relax POSIX requirements \u2013 increase throughput POSIX - Portable Operating System Interface, an IEEE standard designed to facilitate application portability Summary Single NameNode a master server that manages the file system namespace and regulates access to files by clients Multiple DataNotes typically one per node in the cluster Functions Manage storage Serving read/write requests from clients Block creation, deletion, replication based on instructions from NameNode","title":"Overview"},{"location":"Apache/HDFS/#hdfs-hdfs2","text":"HDFS has single NameNode and multiple DataNodes. HDFS2 improvements: HDFS Federation benefits increased namespace scalability performance isolation Multiple Namenode servers Multiple namespaces High Availability (redundant NameNodes) Heterogeneous Storage and Archival Storage (ARCHIVE, DISK, SSD, RAM_DISK)","title":"HDFS -&gt; HDFS2"},{"location":"Apache/HDFS/#write-process","text":"write process is initiated by a client Data gets cached on the client NameNode contacted once a block of data is accumulated NameNode responds with list of DataNodes NameNode is Rack aware Rack awareness is the knowledge of network structure(topology). i.e. location of different data node across the Hadoop cluster. 1stDataNode receives data, writes to local and forwards to 2nd DataNode ... NameNode commits file creation into persistent store. NameNode receives heartbeat and block reports from all DataNodes.","title":"Write Process"},{"location":"Apache/HDFS/#read-process","text":"Client gets DataNode list from NameNode Read from replica closest to reader","title":"Read Process"},{"location":"Apache/HDFS/#performance-envelope","text":"HDFS block size default block size is 64MB good for large files Importance of #blocks in a file NameNode memory usage Every block represented as object (default replication this will be further increased 3X) Number of map tasks data typically processed block at a time A lot of #small files Impact on NameNode Memory usage ~150 bytes per object 1 billion objects => 300GB memory! Network load Number of checks with DataNodes proportional to number of blocks Performance Impact Map tasks depends on #blocks 10GB of data, 32k file size => 327680 map tasks \u21d2lots of queued tasks \u21d2large overhead of spin up/tear down for each task (latency) \u21d2Inefficient disk I/O with small sizes Large files lots of small files is bad! Solutions Merge/Concatenate files Sequence files HBase, HIVE configuration CombineFileInputFormat optimizes maps","title":"Performance Envelope"},{"location":"Apache/HDFS/#tuning-parameters","text":"Parameters in hdfs-site.xml Block Size default 64MB typically bumped up to 128MB parameter: dfs.blocksize, dfs.block.size Replication default is 3 parameter: dfs.replication tradeoffs Lower it to reduce replication cost, less robust Higher replication can make data local to more workers node count map task Full list dfs.datanode.handler.count (10): Sets the number of server threads on each datanode. dfs.namenode.fs-limits.max-blocks-per-file: Maximum number of blocks per file.","title":"Tuning Parameters"},{"location":"Apache/HDFS/#robustness","text":"NameNode receives heartbeat and block reports from DataNodes Replication trade off w.r.t robustness Might lose a node or local disk during the run \u2013 cannot recover if there is no replication. If there is data corruption of a block from one of the DataNodes \u2013 again cannot recover without replication.","title":"Robustness"},{"location":"Apache/HDFS/#common-failures-mitigations","text":"Common Failures DataNode Failures: Server can fail, disk can crash, data corruption. Network Failures NameNode Failures: Disk failure, node failure Mitigation of Common Failures Periodic heartbeat: from DataNode to NameNode. For DataNodes without recent heartbeat Marked dead, no new IO sent Blocks below replication factor re-replicated on other nodes. Data Corruption Checksum computed on file creation Checksums stored in HDFS namespace. Used to check retrieved data, re-read from alternate replica if need. Multiple copies of central meta data structures. Failover to standby NameNode \u2013 manual by default.","title":"Common Failures &amp; Mitigations"},{"location":"Apache/HDFS/#hdfs-access","text":"HDFS command invoked via bin/hdfs script user commands - filesystem shell commands administrator commands debug commands HDFS NFS Gateway Mount HDFS as a filesystem on the client Browse files using regular filesystem commands Upload/download files from HDFS Stream data to HDFS Apache Flume collecting, aggregating streaming data and moving into HDFS Apache Sqoop Bulk transfers between Hadoop and datastores.","title":"HDFS Access"},{"location":"Apache/HDFS/#hdfs-apis","text":"","title":"HDFS APIs"},{"location":"Apache/HDFS/#native-java-api-for-hdfs","text":"Base class : org.apache.hadoop.fs.FileSystem Important classes FSDataInputStream read : read bytes readFully : read from stream to buffer seek: seek to given offset getPos: get current position in stream FSDataOutputStream getPos: get current position in stream hflush: flush out the data in client's user buffer close: close the underlying output stream Methods : get, open, create Reading from HDFS using API get an instance of FileSystem FileSystem fs = FileSystem.get(URI.create(uri),conf); Open an input stream in = fs.open(new Path(uri)); Use IO utilities to copy from input stream IOUtils.copyBytes(in, System.out,4096,false); Close the stream IOUtils.closeStream(in) Writing to HDFS using API get an instance of FileSystem FileSystem fs = FileSystem.get(URI.create(outuri),conf); Create a file out = fs.create(new Path(outuri)); Write to output stream out.write(buffer, 0, nbytes); Close the file out.close();","title":"Native Java API for HDFS**"},{"location":"Apache/HDFS/#c-api-for-hdfs","text":"libhdfs, header file (hdfs.h)","title":"C API for HDFS"},{"location":"Apache/HDFS/#webhdfs-rest-api","text":"Enabling WebHDFS in hdfs-site.xml dfs.webhdfs.enabled dfs.web.authentication.kerberos.principal dfs.web.authentication.kerberos.keytab HTTP Operations HTTP GET: file status, checksums, attributes HTTP PUT: create, change ownership, rename, permissions,snapshot HTTP POST: append, concat HTTP DELETE: Delete files, snapshot","title":"WebHDFS REST API"},{"location":"Apache/MapReduce/","text":"MapReduce A layer of software to help you bring computation to the data and organize the output Framework user defines pair mapper & reducer functions Hadoop handles the logistics: shuffle, group, distribute Flow User defines a map function map() map() reads data and outputs <key,value> User defines a reduce function reduce() reduce() reads <key,value> and outputs result Principle In general 1 mapper per data split (typically) 1 reducer per computer core (best parallelism) Composite \\<keys> Extra info in \\<values> Cascade Map/Reduce jobs bin keys into ranges to reduce computational cost N keys into R groups if size (N/R) increases shuffle cost increases reducer complexity decreases Aggregate map output when possible (combiner option) Joining Data Combine datasets by key A standard data management function Joins can be inner, left or right outer Summary Task Decomposition mappers are separate and independent mappers work on data parts Common mappers Filter (subset data) Identity (just pass data) Splitter (as for counting) Limitations Must fit paradigm Map/Reduce data not persistent Requires programming/debugging Not interactive Force pipeline into Map and Reduce steps (cannot accommodate map-reduce-map .etc) Read from disk for each MapReduce job (bad for iterative algorithms, i.e. machine learning)","title":"MapReduce"},{"location":"Apache/MapReduce/#mapreduce","text":"A layer of software to help you bring computation to the data and organize the output","title":"MapReduce"},{"location":"Apache/MapReduce/#framework","text":"user defines pair mapper & reducer functions Hadoop handles the logistics: shuffle, group, distribute","title":"Framework"},{"location":"Apache/MapReduce/#flow","text":"User defines a map function map() map() reads data and outputs <key,value> User defines a reduce function reduce() reduce() reads <key,value> and outputs result","title":"Flow"},{"location":"Apache/MapReduce/#principle","text":"In general 1 mapper per data split (typically) 1 reducer per computer core (best parallelism) Composite \\<keys> Extra info in \\<values> Cascade Map/Reduce jobs bin keys into ranges to reduce computational cost N keys into R groups if size (N/R) increases shuffle cost increases reducer complexity decreases Aggregate map output when possible (combiner option)","title":"Principle"},{"location":"Apache/MapReduce/#joining-data","text":"Combine datasets by key A standard data management function Joins can be inner, left or right outer","title":"Joining Data"},{"location":"Apache/MapReduce/#summary","text":"Task Decomposition mappers are separate and independent mappers work on data parts Common mappers Filter (subset data) Identity (just pass data) Splitter (as for counting)","title":"Summary"},{"location":"Apache/MapReduce/#limitations","text":"Must fit paradigm Map/Reduce data not persistent Requires programming/debugging Not interactive Force pipeline into Map and Reduce steps (cannot accommodate map-reduce-map .etc) Read from disk for each MapReduce job (bad for iterative algorithms, i.e. machine learning)","title":"Limitations"},{"location":"Apache/Spark/","text":"Spark born at UC Berkeley, managed by Apache Advantages over MapReduce ~20 highly efficient distributed operations, any combination of them good for iterative algorithms, i.e. machine learning, by in-memory caching of data Native Python, Scala, R interfaces; interactive shells Architecture Master Node Driver Program Spark Context (object, gateway to connect spark instance and submit jobs) Cluster Manager 2 interfaces YARN Standalone Provision/Restart Workers Worker Node Spark Executor JVM (Java Virtual Machine) <--> HDFS RDD Resilient Distributed Dataset: data containers (immutable) Dataset are created from HDFS, S3, HBase, JSON, text, Local hierarchy of folders transforming another RDD Distributed distributed across the cluster of machines divided in partitions, atomic chunks of data Resilient Recover from errors, e.g. node failure, slow processes Track history of each partition, re-run Create RDD integer_RDD = sc.parallelize(range(10), 3) text_RDD = sc.textFile(\"file:///home/cloudera/testfile1\") text_RDD = sc.textFile(\"/user/cloudera/input/testfile1\") check partitions # Gather all data on the driver integer_RDD.collect() # Maintain splitting in partitions integer_RDD.glom().collect() check data # outputs the first line text_RDD.take(1) wordcount # map def split_words(line): return line.split() def create_pair(word): return (word, 1) pairs_RDD = text_RDD.flatMap(split_words).map(create_pair) # reduce def sum_counts(a, b): return a + b wordcounts_RDD = pairs_RDD.reduceByKey(sum_counts) Transformations RDD are immutable never modify RDD inplace transform RDD to another RDD transformations are lazy (nothing happens straight away) narrow vs wide narrow map() filter() wide groupByKey() reduceByKey(func) repartition(numPartitions) Apply transformation map applys function to each element of RDD, works on partition instead of on element def lower(line): return line.lower() lower_text_RDD = text_RDD.map(lower) flatMap(func) - map then flatten output def split_words(line): return line.split() words_RDD = text_RDD.flatMap(split_words) filter(func) - keep only elements where func is true def starts_with_a(word): return word.lower().startswith(\"a\") words_RDD.filter(starts_with_a).collect() sample(withReplacement, fraction, seed) - get a random data fraction coalesce(numPartitions) - merge partitions to reduce them to numPartitions sc.parallelize(range(10), 4).glom().collect() sc.parallelize(range(10), 4).coalesce(2).glom().collect() groupByKey() - wide transformations of (K, V) pairs to (K, iterable of all V) -- shuffle pairs_RDD.groupByKey().collect() for k,v in pairs_RDD.groupByKey().collect(): print \"Key:\", k, \",Values:\", list(v) reduceByKey(func) - wide transformation of (K, V) pairs to (K, result of reduction by func on all V) repartition(numPartitions) : similar to coalesce, shuffles all data to increase or decrease number of partitions to numPartitions Shuffle Global redistribution of data High impact on performance Process write to local disk requests data over the network DAG Directed Acyclic Graph are used to track dependencies (also known as lineage or provenance). DAG in Spark nodes are RDDs arrows are Transformations to recover lost partitions Actions Final stage of workflow Triggers execution of the DAG collect() and take return results to the Driver or writes to HDFS Examples collect() - copy all elements to the driver take(n) - copy first n elements reduce(func) - aggregate elements with func (takes 2 elements, returns 1) saveAsTextFile(filename) - save to local file or HDFS Memory Caching By default each job re-processes from HDFS Mark RDD with .cache() Lazy When to cache? Generally not the input data Do validation and cleaning Cache for iterative algorithm How to cache? Memory (most common) Disk (rare) Both (for heavy calculations) Speedup Easily 10x or even 100x depending on application Caching is gradual Fault tolerant Broadcast Broadcast variables Large variable used in all nodes Transfer just once per Executor Efficient peer-to-peer transfer as soon as one node gets a chunk of the data, it will copy this variable by sharing its chunk of data with the other executor config = sc.broadcast({\"order\":3, \"filter\":True}) config.value Accumulator Common pattern of accumulating to a variable across the cluster Write-only on nodes accum = sc.accumulator(0) def test_accum(x): accum.add(x) sc.parallelize([1, 2, 3, 4]).foreach(test_accum) accum.value","title":"Spark"},{"location":"Apache/Spark/#spark","text":"born at UC Berkeley, managed by Apache","title":"Spark"},{"location":"Apache/Spark/#advantages","text":"over MapReduce ~20 highly efficient distributed operations, any combination of them good for iterative algorithms, i.e. machine learning, by in-memory caching of data Native Python, Scala, R interfaces; interactive shells","title":"Advantages"},{"location":"Apache/Spark/#architecture","text":"Master Node Driver Program Spark Context (object, gateway to connect spark instance and submit jobs) Cluster Manager 2 interfaces YARN Standalone Provision/Restart Workers Worker Node Spark Executor JVM (Java Virtual Machine) <--> HDFS","title":"Architecture"},{"location":"Apache/Spark/#rdd","text":"Resilient Distributed Dataset: data containers (immutable) Dataset are created from HDFS, S3, HBase, JSON, text, Local hierarchy of folders transforming another RDD Distributed distributed across the cluster of machines divided in partitions, atomic chunks of data Resilient Recover from errors, e.g. node failure, slow processes Track history of each partition, re-run Create RDD integer_RDD = sc.parallelize(range(10), 3) text_RDD = sc.textFile(\"file:///home/cloudera/testfile1\") text_RDD = sc.textFile(\"/user/cloudera/input/testfile1\") check partitions # Gather all data on the driver integer_RDD.collect() # Maintain splitting in partitions integer_RDD.glom().collect() check data # outputs the first line text_RDD.take(1) wordcount # map def split_words(line): return line.split() def create_pair(word): return (word, 1) pairs_RDD = text_RDD.flatMap(split_words).map(create_pair) # reduce def sum_counts(a, b): return a + b wordcounts_RDD = pairs_RDD.reduceByKey(sum_counts)","title":"RDD"},{"location":"Apache/Spark/#transformations","text":"RDD are immutable never modify RDD inplace transform RDD to another RDD transformations are lazy (nothing happens straight away) narrow vs wide narrow map() filter() wide groupByKey() reduceByKey(func) repartition(numPartitions) Apply transformation map applys function to each element of RDD, works on partition instead of on element def lower(line): return line.lower() lower_text_RDD = text_RDD.map(lower) flatMap(func) - map then flatten output def split_words(line): return line.split() words_RDD = text_RDD.flatMap(split_words) filter(func) - keep only elements where func is true def starts_with_a(word): return word.lower().startswith(\"a\") words_RDD.filter(starts_with_a).collect() sample(withReplacement, fraction, seed) - get a random data fraction coalesce(numPartitions) - merge partitions to reduce them to numPartitions sc.parallelize(range(10), 4).glom().collect() sc.parallelize(range(10), 4).coalesce(2).glom().collect() groupByKey() - wide transformations of (K, V) pairs to (K, iterable of all V) -- shuffle pairs_RDD.groupByKey().collect() for k,v in pairs_RDD.groupByKey().collect(): print \"Key:\", k, \",Values:\", list(v) reduceByKey(func) - wide transformation of (K, V) pairs to (K, result of reduction by func on all V) repartition(numPartitions) : similar to coalesce, shuffles all data to increase or decrease number of partitions to numPartitions Shuffle Global redistribution of data High impact on performance Process write to local disk requests data over the network","title":"Transformations"},{"location":"Apache/Spark/#dag","text":"Directed Acyclic Graph are used to track dependencies (also known as lineage or provenance). DAG in Spark nodes are RDDs arrows are Transformations to recover lost partitions","title":"DAG"},{"location":"Apache/Spark/#actions","text":"Final stage of workflow Triggers execution of the DAG collect() and take return results to the Driver or writes to HDFS Examples collect() - copy all elements to the driver take(n) - copy first n elements reduce(func) - aggregate elements with func (takes 2 elements, returns 1) saveAsTextFile(filename) - save to local file or HDFS","title":"Actions"},{"location":"Apache/Spark/#memory-caching","text":"By default each job re-processes from HDFS Mark RDD with .cache() Lazy When to cache? Generally not the input data Do validation and cleaning Cache for iterative algorithm How to cache? Memory (most common) Disk (rare) Both (for heavy calculations) Speedup Easily 10x or even 100x depending on application Caching is gradual Fault tolerant","title":"Memory Caching"},{"location":"Apache/Spark/#broadcast","text":"Broadcast variables Large variable used in all nodes Transfer just once per Executor Efficient peer-to-peer transfer as soon as one node gets a chunk of the data, it will copy this variable by sharing its chunk of data with the other executor config = sc.broadcast({\"order\":3, \"filter\":True}) config.value Accumulator Common pattern of accumulating to a variable across the cluster Write-only on nodes accum = sc.accumulator(0) def test_accum(x): accum.add(x) sc.parallelize([1, 2, 3, 4]).foreach(test_accum) accum.value","title":"Broadcast"},{"location":"Apache/hadoop/","text":"Hadoop Framework Basic Modules Common (libraries) HDFS (file system) YARN (resource manager & job scheduler) MapReduce (programming model to scale data across different processes) Improvements MapReduce -> YARN Separate resource management and job scheduling/monitoring. YARN improvements global ResourceManager NodeManager on each node ApplicationMaster per application High Availability ResouceManager Timeline Server Use of Cgroups Secure Containers web services REST APIs HDFS -> HDFS2 HDFS has single NameNode and multiple DataNodes. HDFS2 improvements HDFS Federation benefits increased namespace scalability performance isolation Multiple Namenode servers Multiple namespaces High Availability (redundant NameNodes) Heterogeneous Storage and Archival Storage (ARCHIVE, DISK, SSD, RAM_DISK) Hadoop -> Spark Spark is multi-stage in-memory programming Hadoop is 2-stage disk based map reduce programming Spark requires a cluster management and a distributed storage system. Hadoop Frameworks supports in-memory caching data Tez Dataflow graphs Custom data types Can run complex DAG of tasks Dynamic DAG changes Resource usage efficiency Spark Advanced DAG execution engine Supports cyclic data flow (good for machine learning) In-memory computing Java, Scala, Python, R Existing optimized libraries Hadoop Resource Scheduling Schedulers are by default - FIFO (queue). Fairshare Scheduler try to balance out the resource allocation across applications over time Balances out resource allocation among apps over time Can organize into queues/sub-queues Guarantee minimum shares Limits per user/app Weighted app priorities Capacity Scheduler guaranteed capacity for each application or group, and there are safeguards to prevent a user or an application from taking down the whole cluster by running it out of resources queues and sub-queues Capacity Guarantee with elasticity ACLs for security An ACL (access control list) provides a way to set different permissions for specific named users or named groups, not only the file's owner and the file's group. Runtime changes/draining apps Resource based scheduling Hadoop-Based Applications Databases / Stores avro data structures within context of Hadoop MapReduce jobs Hbase Scalable data store Non-relational distributed database Runs on top of HDFS Compression In-memory operations: MemStore, BlockCache Features Consistency High Availability Automatic Sharding Replication Security SQL like access (Hive, Spark, Impala) Cassandra distributed data management system Querying Pig Platform for data processing, good for ETL Components Pig Latin: High level language infrastructure layer Execution environment local MapReduce Tez Extensible (can write custom functions) Hive Data warehouse software HiveQL: SQL like language to structure and query data Data in HDFS, HBase Execution environment MapReduce Tez Spark Custom mappers/reducers Table and storage management Beeline Hive command line interface (CLI) HCatalog webHcat (REST API for HCatalog) Impala Spark Machine Learning / Graph Processing Giraph Mahout Spark","title":"Hadoop Framework Basic Modules"},{"location":"Apache/hadoop/#hadoop-framework-basic-modules","text":"Common (libraries) HDFS (file system) YARN (resource manager & job scheduler) MapReduce (programming model to scale data across different processes)","title":"Hadoop Framework Basic Modules"},{"location":"Apache/hadoop/#improvements","text":"","title":"Improvements"},{"location":"Apache/hadoop/#mapreduce-yarn","text":"Separate resource management and job scheduling/monitoring. YARN improvements global ResourceManager NodeManager on each node ApplicationMaster per application High Availability ResouceManager Timeline Server Use of Cgroups Secure Containers web services REST APIs","title":"MapReduce -&gt; YARN"},{"location":"Apache/hadoop/#hdfs-hdfs2","text":"HDFS has single NameNode and multiple DataNodes. HDFS2 improvements HDFS Federation benefits increased namespace scalability performance isolation Multiple Namenode servers Multiple namespaces High Availability (redundant NameNodes) Heterogeneous Storage and Archival Storage (ARCHIVE, DISK, SSD, RAM_DISK)","title":"HDFS -&gt; HDFS2"},{"location":"Apache/hadoop/#hadoop-spark","text":"Spark is multi-stage in-memory programming Hadoop is 2-stage disk based map reduce programming Spark requires a cluster management and a distributed storage system.","title":"Hadoop -&gt; Spark"},{"location":"Apache/hadoop/#hadoop-frameworks","text":"supports in-memory caching data","title":"Hadoop Frameworks"},{"location":"Apache/hadoop/#tez","text":"Dataflow graphs Custom data types Can run complex DAG of tasks Dynamic DAG changes Resource usage efficiency","title":"Tez"},{"location":"Apache/hadoop/#spark","text":"Advanced DAG execution engine Supports cyclic data flow (good for machine learning) In-memory computing Java, Scala, Python, R Existing optimized libraries","title":"Spark"},{"location":"Apache/hadoop/#hadoop-resource-scheduling","text":"Schedulers are by default - FIFO (queue).","title":"Hadoop Resource Scheduling"},{"location":"Apache/hadoop/#fairshare-scheduler","text":"try to balance out the resource allocation across applications over time Balances out resource allocation among apps over time Can organize into queues/sub-queues Guarantee minimum shares Limits per user/app Weighted app priorities","title":"Fairshare Scheduler"},{"location":"Apache/hadoop/#capacity-scheduler","text":"guaranteed capacity for each application or group, and there are safeguards to prevent a user or an application from taking down the whole cluster by running it out of resources queues and sub-queues Capacity Guarantee with elasticity ACLs for security An ACL (access control list) provides a way to set different permissions for specific named users or named groups, not only the file's owner and the file's group. Runtime changes/draining apps Resource based scheduling","title":"Capacity Scheduler"},{"location":"Apache/hadoop/#hadoop-based-applications","text":"","title":"Hadoop-Based Applications"},{"location":"Apache/hadoop/#databases-stores","text":"","title":"Databases / Stores"},{"location":"Apache/hadoop/#avro","text":"data structures within context of Hadoop MapReduce jobs","title":"avro"},{"location":"Apache/hadoop/#hbase","text":"Scalable data store Non-relational distributed database Runs on top of HDFS Compression In-memory operations: MemStore, BlockCache Features Consistency High Availability Automatic Sharding Replication Security SQL like access (Hive, Spark, Impala)","title":"Hbase"},{"location":"Apache/hadoop/#cassandra","text":"distributed data management system","title":"Cassandra"},{"location":"Apache/hadoop/#querying","text":"","title":"Querying"},{"location":"Apache/hadoop/#pig","text":"Platform for data processing, good for ETL Components Pig Latin: High level language infrastructure layer Execution environment local MapReduce Tez Extensible (can write custom functions)","title":"Pig"},{"location":"Apache/hadoop/#hive","text":"Data warehouse software HiveQL: SQL like language to structure and query data Data in HDFS, HBase Execution environment MapReduce Tez Spark Custom mappers/reducers Table and storage management Beeline Hive command line interface (CLI) HCatalog webHcat (REST API for HCatalog)","title":"Hive"},{"location":"Apache/hadoop/#impala","text":"","title":"Impala"},{"location":"Apache/hadoop/#spark_1","text":"","title":"Spark"},{"location":"Apache/hadoop/#machine-learning-graph-processing","text":"","title":"Machine Learning / Graph Processing"},{"location":"Apache/hadoop/#giraph","text":"","title":"Giraph"},{"location":"Apache/hadoop/#mahout","text":"","title":"Mahout"},{"location":"Apache/hadoop/#spark_2","text":"","title":"Spark"},{"location":"devops/dev_env/","text":"Operating System Windows Mac OS Linux Unix Software Package Management System APT Advanced Package Tool Homebrew for MacOS or Linux, written in Ruby, initiated in 2009 PIP RPM Snappy Steam Others List of software package management systems Software Packages Developer Tool IntelliJ Anaconda","title":"Operating System"},{"location":"devops/dev_env/#operating-system","text":"","title":"Operating System"},{"location":"devops/dev_env/#windows","text":"","title":"Windows"},{"location":"devops/dev_env/#mac-os","text":"","title":"Mac OS"},{"location":"devops/dev_env/#linux","text":"","title":"Linux"},{"location":"devops/dev_env/#unix","text":"","title":"Unix"},{"location":"devops/dev_env/#software-package-management-system","text":"","title":"Software Package Management System"},{"location":"devops/dev_env/#apt","text":"Advanced Package Tool","title":"APT"},{"location":"devops/dev_env/#homebrew","text":"for MacOS or Linux, written in Ruby, initiated in 2009","title":"Homebrew"},{"location":"devops/dev_env/#pip","text":"","title":"PIP"},{"location":"devops/dev_env/#rpm","text":"","title":"RPM"},{"location":"devops/dev_env/#snappy","text":"","title":"Snappy"},{"location":"devops/dev_env/#steam","text":"","title":"Steam"},{"location":"devops/dev_env/#others","text":"List of software package management systems","title":"Others"},{"location":"devops/dev_env/#software-packages","text":"","title":"Software Packages"},{"location":"devops/dev_env/#developer-tool","text":"","title":"Developer Tool"},{"location":"devops/dev_env/#intellij","text":"","title":"IntelliJ"},{"location":"devops/dev_env/#anaconda","text":"","title":"Anaconda"},{"location":"devops/git/","text":"clone a repo git clone $@git git remote -v # add/delete upstream git remote add upstream $git:.git git remote rm upstream # rename git remote set-url origin $.git git remote set-url upstream $.git create a branch git checkout -b $branch git branch -d basic commands git branch git remote -v git status git diff $file git clean -n # check which files will be deleted before actually deleting git clean -f # delete untracked files create pull request git add $file git add . git commit -m \"message\" git push origin $branch git push origin $current_branch:remote_branch resolve a conflict to be validated git checkout master git remote -v git pull upstream master git push origin master git checkout $branch # get back to the working branch git rebase master git am --show-current-patch vi $file git add $file git rebase --continue git push origin $branch -f origin vs upstream in general upstream = original repo origin = fork git fetch alone would fetch from origin by default What is the difference between origin and upstream on GitHub?","title":"Git"},{"location":"devops/git/#clone-a-repo","text":"git clone $@git git remote -v # add/delete upstream git remote add upstream $git:.git git remote rm upstream # rename git remote set-url origin $.git git remote set-url upstream $.git","title":"clone a repo"},{"location":"devops/git/#create-a-branch","text":"git checkout -b $branch git branch -d","title":"create a branch"},{"location":"devops/git/#basic-commands","text":"git branch git remote -v git status git diff $file git clean -n # check which files will be deleted before actually deleting git clean -f # delete untracked files","title":"basic commands"},{"location":"devops/git/#create-pull-request","text":"git add $file git add . git commit -m \"message\" git push origin $branch git push origin $current_branch:remote_branch","title":"create pull request"},{"location":"devops/git/#resolve-a-conflict","text":"to be validated git checkout master git remote -v git pull upstream master git push origin master git checkout $branch # get back to the working branch git rebase master git am --show-current-patch vi $file git add $file git rebase --continue git push origin $branch -f","title":"resolve a conflict"},{"location":"devops/git/#origin-vs-upstream","text":"in general upstream = original repo origin = fork git fetch alone would fetch from origin by default What is the difference between origin and upstream on GitHub?","title":"origin vs upstream"}]}